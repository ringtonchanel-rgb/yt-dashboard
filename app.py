import streamlit as st
import pandas as pd
import numpy as np
import io
import re
from datetime import timedelta
import plotly.express as px

st.set_page_config(page_title="Channelytics", layout="wide")

# ------------------ CSS: –∫–∞—Ä—Ç–æ—á–∫–∏/—Å–µ–≥–º–µ–Ω—Ç—ã/—à–∞–ø–∫–∞ ------------------
CUSTOM_CSS = """
<style>
/* –û–±—â–∏–π —Ñ–æ–Ω —á—É—Ç—å —Å–≤–µ—Ç–ª–µ–µ */
section.main > div { padding-top: 0.5rem !important; }

.header-wrap{
  display:flex; align-items:center; gap:14px; margin:8px 0 4px 0;
}
.avatar{
  width:64px;height:64px; border-radius:14px;
  background:linear-gradient(135deg,#49c6ff,#2f79ff);
  display:flex;align-items:center;justify-content:center;
  color:#fff;font-weight:800;font-size:28px;
}
.channel-info h1{margin:0;font-size:22px;line-height:1.1;}
.channel-info .handle{opacity:.7; font-size:14px;}
.badge{background:#f2f4f7;border-radius:999px;padding:4px 10px;font-size:12px;margin-left:6px;}
.sub-badges{display:flex;gap:6px;align-items:center;}

.kpi-row{display:grid;grid-template-columns:repeat(4,1fr);gap:14px;margin:10px 0 2px 0;}
.kpi-card{
  background:#fff;border:1px solid #f0f0f0;border-radius:12px;padding:14px 16px;
  box-shadow:0 1px 3px rgba(16,24,40,.06);
}
.kpi-card h3{margin:0;font-size:12px;opacity:.7;font-weight:600;}
.kpi-value{font-size:26px;font-weight:800;margin-top:6px;}
.kpi-delta{font-size:12px;margin-top:4px;}
.delta-up{color:#12b76a;font-weight:700;}
.delta-down{color:#f04438;font-weight:700;}
.delta-zero{opacity:.6}

.segment{
  background:#fff;border:1px solid #e6e8ec;border-radius:10px;display:inline-flex;gap:0;overflow:hidden;
}
.segment button{
  border:none;padding:8px 12px;font-size:13px;background:transparent;cursor:pointer;
}
.segment button.active{background:#111827;color:#fff;}
.segment button:hover{background:#f5f5f6}

.card{background:#fff;border:1px solid #f0f0f0;border-radius:12px;padding:14px 16px;
      box-shadow:0 1px 3px rgba(16,24,40,.06);}
.card h3{margin:0 0 10px 0;font-size:14px;opacity:.7}
.muted{opacity:.7;font-size:12px}

/* –¥–æ–Ω–∞—Ç-–ø–∏—Ä–æ–≥ —Å–ø—Ä–∞–≤–∞ */
.two-cols{display:grid;grid-template-columns:2fr 1fr;gap:14px;}
</style>
"""
st.write(CUSTOM_CSS, unsafe_allow_html=True)

# ================== –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–• –í STATE ==================
# –ë—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å —Ç–∞–∫:
# st.session_state["groups"] = {
#   group_name: {
#       "reports": [ {"name": str, "df": DataFrame}, ... ],
#       "revenues": [ {"name": str, "df": DataFrame}, ... ],
#       "allow_dups": bool
#   }, ...
# }
# –ü–æ–¥–¥–µ—Ä–∂–∏–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç ({"df": DataFrame}) ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º.
if "groups" not in st.session_state or not isinstance(st.session_state["groups"], dict):
    st.session_state["groups"] = {}

def ensure_group_shape():
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –≥—Ä—É–ø–ø."""
    for g, val in list(st.session_state["groups"].items()):
        if isinstance(val, dict) and "reports" in val:
            continue
        # —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: {"df": df, "allow_dups": bool}
        if isinstance(val, dict) and "df" in val:
            st.session_state["groups"][g] = {
                "reports": [{"name": f"{g}_legacy.csv", "df": val["df"]}],
                "revenues": [],
                "allow_dups": bool(val.get("allow_dups", False)),
            }
        else:
            # —á—Ç–æ-—Ç–æ –Ω–µ —Ç–æ ‚Äî –æ–±–Ω—É–ª–∏–º
            st.session_state["groups"][g] = {"reports": [], "revenues": [], "allow_dups": False}

ensure_group_shape()

def reset_state():
    st.session_state["groups"] = {}
    st.success("State cleared.")

# ================== –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø / –ü–ê–†–°–ò–ù–ì ==================
def _norm(s: str) -> str:
    return str(s).strip().lower()

COLMAP = {
    "publish_time": ["video publish time","publish time","publish date","upload date","–≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏","–¥–∞—Ç–∞"],
    "title": ["title","video title","–Ω–∞–∑–≤–∞–Ω–∏–µ","content","–∫–æ–Ω—Ç–µ–Ω—Ç"],
    "video_id": ["video id","id","–∏–¥"],
    "video_link": ["youtube link","link","—Å—Å—ã–ª–∫–∞","url"],
    "views": ["views","–ø—Ä–æ—Å–º–æ—Ç—Ä—ã"],
    "impressions": ["impressions","–ø–æ–∫–∞–∑—ã"],
    "ctr": ["ctr","impressions click-through rate","ctr –¥–ª—è –∑–Ω–∞—á–∫–æ–≤"],
    "watch_hours": ["watch time (hours)","–≤—Ä–µ–º—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (—á–∞—Å—ã)"],
    "watch_minutes": ["watch time (minutes)","–≤—Ä–µ–º—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (–º–∏–Ω)"],
    "duration": ["duration","–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"],
    "shorts": ["shorts","is shorts","—à–æ—Ä—Ç—Å","–∫–æ—Ä–æ—Ç–∫–æ–µ –≤–∏–¥–µ–æ"],
    "format": ["format","—Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞"],
    "revenue": ["estimated revenue","estimated partner revenue","–¥–æ—Ö–æ–¥"]
}

def find_col(df, names):
    pool = {_norm(c): c for c in df.columns}
    for n in names:
        n = _norm(n)
        if n in pool: return pool[n]
    for n in names:
        n = _norm(n)
        for c in df.columns:
            if n in _norm(c): return c
    return None

def detect_columns(df): return {k: find_col(df, v) for k, v in COLMAP.items()}

def to_num(x):
    if x is None: return np.nan
    if isinstance(x,(int,float,np.number)): return float(x)
    s = str(x).strip().replace("\u202f","").replace("\xa0","").replace(" ", "")
    if s.endswith("%"): s = s[:-1]
    if "," in s and "." not in s: s = s.replace(",", ".")
    try: return float(s)
    except: return np.nan

def parse_duration_to_seconds(val):
    if pd.isna(val): return np.nan
    s = str(val).strip()
    if s.isdigit(): return float(s)
    m = re.match(r'(?:(\d+)\s*h)?\s*(?:(\d+)\s*m)?\s*(?:(\d+)\s*s)?', s, re.I)
    if m and any(m.groups()):
        h = int(m.group(1)) if m.group(1) else 0
        mm = int(m.group(2)) if m.group(2) else 0
        ss = int(m.group(3)) if m.group(3) else 0
        return float(h*3600 + mm*60 + ss)
    parts = s.split(":")
    try:
        if len(parts) == 3:
            h, mm, ss = map(int, parts); return float(h*3600 + mm*60 + ss)
        if len(parts) == 2:
            mm, ss = map(int, parts);    return float(mm*60 + ss)
    except: pass
    return np.nan

def parse_one_file(uploaded_file, allow_dups=True):
    """–ü–∞—Ä—Å–∏–º –û–î–ò–ù —Ñ–∞–π–ª -> –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π df + –æ—Ç—á—ë—Ç –æ —Ö–æ–¥–µ."""
    raw = uploaded_file.getvalue()
    df = None
    for enc in (None, "utf-8-sig", "cp1251"):
        try:
            df = pd.read_csv(io.BytesIO(raw), encoding=enc) if enc else pd.read_csv(io.BytesIO(raw))
            break
        except: pass
    if df is None or df.empty:
        return None, f"‚ùå {uploaded_file.name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV"

    cols = detect_columns(df)
    if not cols["publish_time"]:
        return None, f"‚ö†Ô∏è {uploaded_file.name}: –Ω–µ—Ç –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é"

    out = pd.DataFrame()
    out["publish_time"] = pd.to_datetime(df[cols["publish_time"]], errors="coerce")
    out = out.dropna(subset=["publish_time"])
    if cols["title"]: out["title"] = df[cols["title"]].astype(str)
    if cols["video_id"]: out["video_id"] = df[cols["video_id"]].astype(str).str.strip()
    if cols["video_link"]: out["video_link"] = df[cols["video_link"]].astype(str)

    if cols["views"]: out["views"] = pd.to_numeric(df[cols["views"]].apply(to_num), errors="coerce")
    if cols["impressions"]: out["impressions"] = pd.to_numeric(df[cols["impressions"]].apply(to_num), errors="coerce")
    if cols["ctr"]:
        out["ctr"] = pd.to_numeric(df[cols["ctr"]].apply(to_num), errors="coerce")
        # –µ—Å–ª–∏ CTR ‚â§ 1 -> –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–æ–ª—é –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        if out["ctr"].dropna().max() <= 1.0:
            out["ctr"] = out["ctr"] * 100.0
    if cols["watch_hours"]:
        out["watch_hours"] = pd.to_numeric(df[cols["watch_hours"]].apply(to_num), errors="coerce")
    elif cols["watch_minutes"]:
        out["watch_hours"] = pd.to_numeric(df[cols["watch_minutes"]].apply(to_num), errors="coerce")/60.0

    # duration_sec
    if cols["duration"]:
        dur_raw = df[cols["duration"]].astype(str).str.strip()
        out["duration_sec"] = dur_raw.apply(parse_duration_to_seconds)
    else:
        out["duration_sec"] = np.nan

    # —Ñ–æ—Ä–º–∞—Ç: vertical/horizontal
    out["format"] = np.nan
    if cols["shorts"]:
        short_col = df[cols["shorts"]].astype(str).str.lower()
        out.loc[short_col.isin(["1","true","–¥–∞","yes","y","short","shorts"]), "format"] = "vertical"
    if cols["format"]:
        fmt_col = df[cols["format"]].astype(str).str.lower()
        out.loc[fmt_col.str.contains("short"), "format"] = "vertical"
    out.loc[out["format"].isna() & (out["duration_sec"] <= 60), "format"] = "vertical"
    out["format"] = out["format"].fillna("horizontal")

    # –≤–æ–∑–º–æ–∂–Ω–æ –µ—Å—Ç—å revenue –≤ —Å–∞–º–æ–º –æ—Ç—á—ë—Ç–µ
    if cols["revenue"]:
        out["revenue"] = pd.to_numeric(df[cols["revenue"]].apply(to_num), errors="coerce")

    if not allow_dups and "title" in out:
        out = out.drop_duplicates(subset=["title","publish_time"])

    out["pub_date"] = out["publish_time"].dt.date
    return out, f"‚úÖ {uploaded_file.name}: {out.shape[0]} —Å—Ç—Ä–æ–∫"

# ----- –ü–æ–¥–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–æ–≤ -----
def attach_revenue(base_df: pd.DataFrame, revenue_packs):
    """
    revenue_packs: —Å–ø–∏—Å–æ–∫ {"name":..., "df": DataFrame} –≥–¥–µ df —Å–æ–¥–µ—Ä–∂–∏—Ç:
    - (video_id, revenue)  –ò–õ–ò
    - (date, revenue)      -> —Ç–æ–≥–¥–∞ –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞)
    """
    if not revenue_packs: return base_df
    df = base_df.copy()
    df["revenue_ext"] = np.nan

    for pack in revenue_packs:
        r = pack.get("df")
        if not isinstance(r, pd.DataFrame): continue
        cols = [c.lower() for c in r.columns]
        if "video_id" in cols and "revenue" in cols:
            r2 = r.rename(columns={r.columns[cols.index("video_id")]: "video_id",
                                   r.columns[cols.index("revenue")]: "revenue"}).copy()
            r2["video_id"] = r2["video_id"].astype(str).str.strip()
            r2["revenue"] = r2["revenue"].apply(to_num)
            df = df.merge(r2[["video_id","revenue"]], on="video_id", how="left", suffixes=("", "_ext"))
            df["revenue_ext"] = df["revenue_ext"].fillna(df["revenue_ext"])
        elif "date" in cols and "revenue" in cols:
            r2 = r.rename(columns={r.columns[cols.index("date")]: "date",
                                   r.columns[cols.index("revenue")]: "revenue"}).copy()
            r2["date"] = pd.to_datetime(r2["date"], errors="coerce")
            daily = r2.groupby("date", as_index=False)["revenue"].sum()
            if "publish_time" in df.columns:
                df["pub_date_dt"] = pd.to_datetime(df["pub_date"])
                df = df.merge(daily, left_on="pub_date_dt", right_on="date", how="left", suffixes=("", "_rday"))
                df["revenue_ext"] = df["revenue_ext"].fillna(df["revenue_rday"])
                df.drop(columns=["date","pub_date_dt","revenue_rday"], inplace=True, errors="ignore")

    if "revenue" in df.columns:
        df["revenue_final"] = df["revenue"].fillna(df["revenue_ext"])
    else:
        df["revenue_final"] = df["revenue_ext"]
    return df

# ================== Sidebar: –ù–∞–≤–∏–≥–∞—Ü–∏—è + –≥—Ä—É–ø–ø—ã ==================
st.sidebar.markdown("### üìä YouTube Analytics Tools")
page = st.sidebar.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", ["Channelytics", "Manage Groups"], index=0)

with st.sidebar.expander("‚ûï –î–æ–±–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –≥—Ä—É–ø–ø—É", expanded=(page=="Manage Groups")):
    with st.form("add_group_form", clear_on_submit=False):
        gname = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã (–∫–∞–Ω–∞–ª–∞)", value="")
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –æ—Ç—á—ë—Ç—ã (1..N)", type=["csv"], accept_multiple_files=True)
        uploaded_rev = st.file_uploader("CSV —Å –¥–æ—Ö–æ–¥–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ)", type=["csv"], accept_multiple_files=True)
        allow_dups = st.checkbox("–†–∞–∑—Ä–µ—à–∞—Ç—å –¥—É–±–ª–∏ –≤ –æ—Ç—á—ë—Ç–∞—Ö", value=False)
        ok = st.form_submit_button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å")
    if ok:
        if not gname.strip():
            st.warning("–î–∞–π—Ç–µ –∏–º—è –≥—Ä—É–ø–ø–µ.")
        elif not uploaded and not uploaded_rev:
            st.warning("–ü—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª (–æ—Ç—á—ë—Ç –∏–ª–∏ –¥–æ—Ö–æ–¥).")
        else:
            st.session_state["groups"].setdefault(gname, {"reports": [], "revenues": [], "allow_dups": allow_dups})
            st.session_state["groups"][gname]["allow_dups"] = allow_dups

            # –æ—Ç—á—ë—Ç—ã
            for uf in uploaded or []:
                df_parsed, note = parse_one_file(uf, allow_dups=allow_dups)
                st.write(note)
                if df_parsed is not None and not df_parsed.empty:
                    st.session_state["groups"][gname]["reports"].append({"name": uf.name, "df": df_parsed})

            # –¥–æ—Ö–æ–¥—ã
            for rf in uploaded_rev or []:
                raw = rf.getvalue()
                try:
                    rdf = pd.read_csv(io.BytesIO(raw))
                except Exception:
                    rdf = None
                if isinstance(rdf, pd.DataFrame) and not rdf.empty:
                    st.session_state["groups"][gname]["revenues"].append({"name": rf.name, "df": rdf})
                    st.write(f"üí∞ –î–æ—Ö–æ–¥: {rf.name} –∑–∞–≥—Ä—É–∂–µ–Ω ({len(rdf)} —Å—Ç—Ä–æ–∫).")
                else:
                    st.write(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ—Ö–æ–¥: {rf.name}")

            st.success(f"–ì—Ä—É–ø–ø–∞ ¬´{gname}¬ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞.")

groups = st.session_state["groups"]
group_names = sorted(groups.keys())

# ================== KPI / –£—Ç–∏–ª–∏—Ç—ã ==================
def kpi_for_df(dff):
    v = dff["views"].sum() if "views" in dff else np.nan
    imp = dff["impressions"].sum() if "impressions" in dff else np.nan
    ctr = dff["ctr"].dropna().mean() if "ctr" in dff else np.nan
    return v, imp, ctr

def period_slice(df, end_date, days):
    if days == 0:  # Max
        return df, None
    start = end_date - timedelta(days=days)
    return df[df["publish_time"].between(start, end_date)], (start, end_date)

def previous_slice(df, end_date, days):
    if days == 0: return None, None
    start_prev = end_date - timedelta(days=days*2)
    end_prev = end_date - timedelta(days=days)
    return df[df["publish_time"].between(start_prev, end_prev)], (start_prev, end_prev)

def fmt_int(n):
    try: return f"{int(round(float(n))):,}".replace(",", " ")
    except: return "‚Äî"

def fmt_delta(cur, prev):
    if pd.isna(cur) or pd.isna(prev): return "‚Äî", "delta-zero"
    diff = cur - prev
    if abs(prev) < 1e-9:
        return f"+{fmt_int(diff)}", "delta-up" if diff>0 else "delta-down"
    pct = diff/prev*100
    if diff>0: return f"+{fmt_int(diff)} (+{pct:.1f}%)", "delta-up"
    if diff<0: return f"{fmt_int(diff)} ({pct:.1f}%)", "delta-down"
    return "0 (0%)", "delta-zero"

def apply_format_filter(df, fmt_value):
    if fmt_value == "vertical":
        return df[df["format"] == "vertical"] if "format" in df else df
    if fmt_value == "horizontal":
        return df[df["format"] == "horizontal"] if "format" in df else df
    return df

# ================== CHANNELYTICS ==================
if page == "Channelytics":
    st.markdown("‚ö†Ô∏è _–í–∞–∂–Ω–æ: 7D/28D/‚Ä¶ –∑–¥–µ—Å—å ‚Äî **–ø–æ –¥–∞—Ç–∞–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Ä–æ–ª–∏–∫–æ–≤**, –∞ –Ω–µ –ø–æ –¥–∞—Ç–∞–º –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (–∫–∞–∫ –≤ –Ω–∞—Ç–∏–≤–Ω–æ–π YouTube Analytics)._")

    if not group_names:
        st.info("–î–æ–±–∞–≤—å—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –≥—Ä—É–ø–ø—É –≤–æ –≤–∫–ª–∞–¥–∫–µ **Manage Groups**.")
        st.stop()

    colA, colB = st.columns([3,1])
    with colA:
        g = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–Ω–∞–ª/–≥—Ä—É–ø–ø—É", group_names, index=0)
    with colB:
        rpm = st.number_input("RPM ($ –Ω–∞ 1000 –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤)", min_value=0.0, max_value=200.0, value=2.0, step=0.5)

    group = groups[g]
    reports = group["reports"]
    revpacks = group.get("revenues", [])

    if not reports:
        st.warning("–í —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ –Ω–µ—Ç –æ—Ç—á—ë—Ç–æ–≤.")
        st.stop()

    # ---------- ¬´–®–∞–ø–∫–∞¬ª –∫–∞–Ω–∞–ª–∞ ----------
    initials = "".join([w[0] for w in re.sub(r"[^A-Za-z–ê-–Ø–∞-—è0-9 ]","", g).split()[:2]]).upper() or "YT"
    st.markdown(
        f"""
        <div class="header-wrap">
          <div class="avatar">{initials}</div>
          <div class="channel-info">
            <h1>{g}</h1>
            <div class="handle">@{re.sub(r'\\W','', g.lower())}</div>
          </div>
          <div class="sub-badges"><span class="badge">Channelytics</span></div>
        </div>
        """, unsafe_allow_html=True
    )

    # ---------- —Ñ–æ—Ä–º–∞—Ç-—Ñ–∏–ª—å—Ç—Ä ----------
    fmt_filter = st.radio("–§–æ—Ä–º–∞—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞", ["all","horizontal","vertical"], horizontal=True, index=0)

    # ---------- —Å–µ–≥–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ ----------
    # –±–µ—Ä—ë–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π publish_time –ø–æ –í–°–ï–ú —Ñ–∞–π–ª–∞–º
    all_pub = pd.concat([p["df"][["publish_time"]] for p in reports if "publish_time" in p["df"]], ignore_index=True)
    today = all_pub["publish_time"].max() if not all_pub.empty else pd.Timestamp.today()

    seg = st.session_state.get("seg", "28D")
    seg_cols = st.columns([1,1,1,1,1,6])
    with seg_cols[0]:
        if st.button("7D", key="seg7", use_container_width=True): seg="7D"
    with seg_cols[1]:
        if st.button("28D", key="seg28", use_container_width=True): seg="28D"
    with seg_cols[2]:
        if st.button("3M", key="seg3m", use_container_width=True): seg="3M"
    with seg_cols[3]:
        if st.button("1Y", key="seg1y", use_container_width=True): seg="1Y"
    with seg_cols[4]:
        if st.button("Max", key="segmax", use_container_width=True): seg="Max"
    st.session_state["seg"] = seg
    days_map = {"7D":7, "28D":28, "3M":90, "1Y":365, "Max":0}
    days = days_map[seg]

    # ---------- –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –û–¢–ß–Å–¢–ê–ú (–ù–ï —Å—É–º–º–∏—Ä—É–µ–º) ----------
    st.subheader("–°–≤–æ–¥–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –æ—Ç—á—ë—Ç—É (–±–µ–∑ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)")
    rows = []
    combined_cur = []  # –¥–ª—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—â–µ–≥–æ KPI/–≥—Ä–∞—Ñ–∏–∫–æ–≤
    combined_prev = []

    for pack in reports:
        df0 = attach_revenue(pack["df"], revpacks)
        df0 = apply_format_filter(df0, fmt_filter)

        cur, _ = period_slice(df0, today, days)
        prev, _ = previous_slice(df0, today, days)

        v, i, c = kpi_for_df(cur)
        rows.append({
            "–û—Ç—á—ë—Ç": pack["name"],
            "–í–∏–¥–µ–æ (—à—Ç.)": len(cur),
            "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": v,
            "–ü–æ–∫–∞–∑—ã": i,
            "CTR, % (—Å—Ä.)": c,
            "–î–æ—Ö–æ–¥": cur["revenue_final"].sum() if "revenue_final" in cur else np.nan
        })

        combined_cur.append(cur)
        if prev is not None:
            combined_prev.append(prev)

    seg_df = pd.DataFrame(rows)
    # —Å–∫—Ä—ã—Ç—å –¥–æ—Ö–æ–¥, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if "–î–æ—Ö–æ–¥" in seg_df and seg_df["–î–æ—Ö–æ–¥"].notna().sum() == 0:
        seg_df.drop(columns=["–î–æ—Ö–æ–¥"], inplace=True)

    st.dataframe(
        seg_df.style.format({
            "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã":"{:,.0f}", "–ü–æ–∫–∞–∑—ã":"{:,.0f}", "CTR, % (—Å—Ä.)":"{:.2f}"
        }).hide(axis="index"),
        use_container_width=True, height=280
    )

    # ---------- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ–±—â–∏–π KPI –∏ –≥—Ä–∞—Ñ–∏–∫–∏ (–¥–ª—è –æ–±–∑–æ—Ä–∞) ----------
    show_combined = st.toggle("–ü–æ–∫–∞–∑–∞—Ç—å –æ–±—â–∏–π –æ–±–∑–æ—Ä (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è KPI/–≥—Ä–∞—Ñ–∏–∫–æ–≤)", value=True)
    if show_combined:
        cur_all = pd.concat(combined_cur, ignore_index=True) if combined_cur else pd.DataFrame()
        prev_all = pd.concat(combined_prev, ignore_index=True) if combined_prev else None

        # KPI –∫–∞—Ä—Ç–æ—á–∫–∏ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ)
        if not cur_all.empty:
            cur_all = apply_format_filter(cur_all, fmt_filter)
            cur_views, cur_impr, cur_ctr = kpi_for_df(cur_all)
            if prev_all is not None and not prev_all.empty:
                prev_views, prev_impr, prev_ctr = kpi_for_df(prev_all)
            else:
                prev_views = prev_impr = prev_ctr = np.nan

            rev_cur = (cur_views/1000.0)*rpm if pd.notna(cur_views) else np.nan
            rev_prev = (prev_views/1000.0)*rpm if pd.notna(prev_views) else np.nan

            dv, cls_v = fmt_delta(cur_views, prev_views)
            di, cls_i = fmt_delta(cur_impr, prev_impr)
            dr, cls_r = fmt_delta(rev_cur, rev_prev)
            dc, cls_c = fmt_delta(cur_ctr, prev_ctr)

            st.markdown('<div class="kpi-row">', unsafe_allow_html=True)
            st.markdown(f"""
              <div class="kpi-card">
                <h3>VIEWS ({seg})</h3>
                <div class="kpi-value">{fmt_int(cur_views)}</div>
                <div class="kpi-delta {cls_v}">{dv}</div>
              </div>
            """, unsafe_allow_html=True)
            st.markdown(f"""
              <div class="kpi-card">
                <h3>IMPRESSIONS ({seg})</h3>
                <div class="kpi-value">{fmt_int(cur_impr)}</div>
                <div class="kpi-delta {cls_i}">{di}</div>
              </div>
            """, unsafe_allow_html=True)
            st.markdown(f"""
              <div class="kpi-card">
                <h3>EST REV ({seg})</h3>
                <div class="kpi-value">${fmt_int(rev_cur)}</div>
                <div class="kpi-delta {cls_r}">{dr}</div>
              </div>
            """, unsafe_allow_html=True)
            if not pd.isna(cur_ctr):
                st.markdown(f"""
                  <div class="kpi-card">
                    <h3>CTR AVG ({seg})</h3>
                    <div class="kpi-value">{round(cur_ctr,2)}%</div>
                    <div class="kpi-delta {cls_c}">{dc}</div>
                  </div>
                """, unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

            # ---------- –¢—Ä–µ–Ω–¥ + –±–æ–∫–æ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ ----------
            df_trend = cur_all.copy()
            if "publish_time" in df_trend:
                if df_trend["publish_time"].dt.normalize().nunique() > 1:
                    freq = "D"
                    df_trend["bucket"] = df_trend["publish_time"].dt.date
                else:
                    freq = "M"
                    df_trend["bucket"] = df_trend["publish_time"].dt.to_period("M").astype(str)
            else:
                st.warning("–ù–µ—Ç –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ‚Äî —Ç—Ä–µ–Ω–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
                freq = None

            st.markdown('<div class="two-cols">', unsafe_allow_html=True)

            # –õ–µ–≤–∞—è ‚Äî –≥—Ä–∞—Ñ–∏–∫
            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown("<h3>Views trend</h3>", unsafe_allow_html=True)
            if freq:
                trend = df_trend.groupby("bucket")["views"].sum().reset_index()
                fig = px.area(trend, x="bucket", y="views", template="simple_white")
                fig.update_layout(height=360, xaxis_title="", yaxis_title="Views")
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–¥–∞.")
            st.markdown('</div>', unsafe_allow_html=True)

            # –ü—Ä–∞–≤–∞—è ‚Äî most recent + long/shorts
            st.markdown('<div>', unsafe_allow_html=True)
            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown("<h3>Most recent video</h3>", unsafe_allow_html=True)
            if not cur_all.empty:
                last = cur_all.sort_values("publish_time", ascending=False).iloc[0]
                title = last.get("title", "‚Äî")
                link = last.get("video_link") or (f"https://www.youtube.com/watch?v={last.get('video_id')}" if pd.notna(last.get("video_id")) else None)
                st.write(f"**{title}**")
                st.write(f"Published: {pd.to_datetime(last['publish_time']).date()}")
                st.write(f"Views: {fmt_int(last.get('views'))}")
                if link:
                    st.markdown(f"[Open on YouTube]({link})")
            else:
                st.write("‚Äî")
            st.markdown('</div>', unsafe_allow_html=True)

            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown("<h3>Long vs Shorts</h3>", unsafe_allow_html=True)
            if "duration_sec" in cur_all:
                short = (cur_all["duration_sec"]<=60).sum()
                lng = (cur_all["duration_sec"]>60).sum()
                pie = pd.DataFrame({"type":["Shorts","Longs"], "count":[short,lng]})
                pfig = px.pie(pie, names="type", values="count", color="type",
                              color_discrete_map={"Shorts":"#ef4444","Longs":"#4f46e5"})
                pfig.update_layout(height=260, legend_title=None)
                st.plotly_chart(pfig, use_container_width=True)
            else:
                st.write("–ù–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Äî –Ω–µ –º–æ–≥—É —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ Longs/Shorts.")
            st.markdown('</div>', unsafe_allow_html=True)

            st.markdown('</div>', unsafe_allow_html=True)  # two-cols

# ================== MANAGE GROUPS ==================
elif page == "Manage Groups":
    st.title("üß∞ Manage Groups")
    if st.button("–°–±—Ä–æ—Å–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"):
        reset_state()
        st.experimental_rerun()

    if not group_names:
        st.info("–ü–æ–∫–∞ –Ω–µ—Ç –≥—Ä—É–ø–ø. –î–æ–±–∞–≤—å—Ç–µ –∏—Ö —Å–ª–µ–≤–∞ –≤ ¬´–î–æ–±–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –≥—Ä—É–ø–ø—É¬ª.")
    else:
        for g in group_names:
            grp = groups[g]
            with st.expander(f"–ì—Ä—É–ø–ø–∞: {g}", expanded=False):
                st.write(f"–û—Ç—á—ë—Ç–æ–≤: **{len(grp['reports'])}**, –¥–æ—Ö–æ–¥–æ–≤: **{len(grp.get('revenues', []) )}**")
                # —Å–ø–∏—Å–æ–∫ –æ—Ç—á—ë—Ç–æ–≤
                for i, pack in enumerate(list(grp["reports"])):
                    st.markdown(f"**–û—Ç—á—ë—Ç:** {pack['name']}  ¬∑  —Å—Ç—Ä–æ–∫: {len(pack['df'])}")
                    st.dataframe(pack["df"].head(30), use_container_width=True)
                    cols = st.columns(3)
                    with cols[0]:
                        if st.button("–£–¥–∞–ª–∏—Ç—å –æ—Ç—á—ë—Ç", key=f"del_rep_{g}_{i}"):
                            groups[g]["reports"].pop(i)
                            st.experimental_rerun()
                    with cols[1]:
                        st.download_button("–°–∫–∞—á–∞—Ç—å CSV", data=pack["df"].to_csv(index=False).encode("utf-8"),
                                           file_name=f"{pack['name']}_normalized.csv", mime="text/csv")
                    with cols[2]:
                        st.caption("")

                st.markdown("---")
                st.write("**–§–∞–π–ª—ã –¥–æ—Ö–æ–¥–æ–≤:**")
                for j, rpack in enumerate(list(grp.get("revenues", []))):
                    st.markdown(f"‚Ä¢ {rpack['name']}  ¬∑  —Å—Ç—Ä–æ–∫: {len(rpack['df'])}")
                    c1, c2 = st.columns(2)
                    with c1:
                        if st.button("–£–¥–∞–ª–∏—Ç—å –¥–æ—Ö–æ–¥", key=f"del_rev_{g}_{j}"):
                            groups[g]["revenues"].pop(j)
                            st.experimental_rerun()
                    with c2:
                        st.download_button("–°–∫–∞—á–∞—Ç—å –¥–æ—Ö–æ–¥ CSV",
                                           data=rpack["df"].to_csv(index=False).encode("utf-8"),
                                           file_name=f"{rpack['name']}",
                                           mime="text/csv")

                st.markdown("---")
                if st.button(f"–£–¥–∞–ª–∏—Ç—å –≤—Å—é –≥—Ä—É–ø–ø—É ¬´{g}¬ª", key=f"del_group_{g}"):
                    groups.pop(g, None)
                    st.experimental_rerun()
